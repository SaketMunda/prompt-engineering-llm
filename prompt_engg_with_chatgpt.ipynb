{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMtNQHrsD10F1UwM65McYqL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaketMunda/prompt-engineering-llm/blob/master/prompt_engg_with_chatgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering with [ChatGPT from OpenAI](https://platform.openai.com/docs/guides/gpt)\n",
        "\n",
        "\n",
        "> What is Prompt Engineering ?\n",
        "\n",
        "*Prompts are basically some instructions given to deep learning models through texts in english or using some identifiers like keywords or special characters as an input to bring results from deep learning model as an output.*\n",
        "\n",
        "*In the market there are some models like ChatGPT, BardAI and Midjourney and many more which accepts prompts as Input and give results based on the input as outputs like a text of paragraphs or set of python code or an image of Monalisa, it's based on the model you're prompting on.*\n",
        "\n",
        "> What are LLMs ?\n",
        "\n",
        "*LLMs, Large Language Model are machine learning models that use deep learning algorithms to process and understand natural language inputs and generate the results in text form or natural language.*\n",
        "\n",
        "\n",
        "> What is ChatGPT ?\n",
        "\n",
        "*ChatGPT is also a LLM which is been trained by OpenAI to follow an instruction in a prompt and provide a detailed response.*\n",
        "\n",
        "*Chat Generative Pre-trained Transformer is the full-form.*\n",
        "\n",
        "Further readings: https://openai.com/chatgpt\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H6gpNmS_N3Jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guidelines for Prompting"
      ],
      "metadata": {
        "id": "thpRv1OkOpLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "\n",
        "Load the API key and relevant python libraries."
      ],
      "metadata": {
        "id": "d7duO89_eD2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbAgrZ-moUct",
        "outputId": "914f4c93-c587-49ec-cc7b-e818d061ca00"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "from getpass import getpass\n",
        "secret = getpass('Enter the openai api key')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsv3hybSoWKX",
        "outputId": "c1439df7-4218-475e-bb9d-89bbd2c74b98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the openai api key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell after running above\n",
        "openai.api_key = secret"
      ],
      "metadata": {
        "id": "IlLt32vzqIXm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Function\n",
        "\n",
        "We will use OpenAI's `gpt-3.5-turbo` model and the chat completions endpoint.\n",
        "\n",
        "This helper function will make it easier to use prompts and look at the generated outputs:"
      ],
      "metadata": {
        "id": "w05vwmlWqYZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model='gpt-3.5-turbo'):\n",
        "  messages = [{'role':'user', 'content': prompt}]\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model=model,\n",
        "      messages=messages,\n",
        "      temperature=0, # this is the degress of randomness of the model's output\n",
        "  )\n",
        "  return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "MMpOZz33t-LA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting Principles\n",
        "\n",
        "**First Principle**, *Write clear and specific instructions*\n",
        "\n",
        "**Second**, *Give the model time to think*"
      ],
      "metadata": {
        "id": "mkMq-hgyuwdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tactics\n",
        "\n",
        "**Tactic 1 : Use delimeters to clearly indicate distinct parts of the input**\n",
        "\n",
        "- Delimiters can be anything like: ```, `\"\"\"`, `<>`, `<tag></tag>`, `:`"
      ],
      "metadata": {
        "id": "PngdrLjku2hC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "You should express what you want a model to do by \\\n",
        "providing instructions that are as clear and \\\n",
        "specific as you can possibly make them. \\\n",
        "This will guide the model towards the desired output, \\\n",
        "and reduce the chances of receiving irrelevant \\\n",
        "or incorrect responses. Don't confuse writing a \\\n",
        "clear prompt with writing a short prompt. \\\n",
        "In many cases, longer prompts provide more clarity \\\n",
        "and context for the model, which can lead to \\\n",
        "more detailed and relevant outputs.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by triple backticks \\\n",
        "into a single sentence.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPml1KLUvNbc",
        "outputId": "26550671-74e2-4d86-b1ab-44f884ab4c11"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To guide a model towards the desired output and reduce irrelevant or incorrect responses, it is important to provide clear and specific instructions, which can be achieved through longer prompts that offer more clarity and context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "64gugfu3vdRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}